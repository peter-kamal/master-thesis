---
title: "Data set creation"
output: html_notebook
---

Last update: 19.08.2023
Author: Peter Kamal (peter.kamal@t-online.de)

This notebook serves to create an analysis-ready data set for the economic part of the thesis. It imports the raw satellite data exported in 'econ_3_satellite_raw_export.js' and the weather data webscraped with 'econ_4_webscraper.py', merges, cleans and exports it. The final data set is available on figshare (accessible through the paper).

First, import the raw data.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(sf)
options(scipen = 999)

dataDir <- "C:/Users/Kamal/OneDrive/TSE/M2 EE/thesis/econ/data/satellite_output/v3/bc_hansen_processed_"

bmtas <- st_read(paste0(dataDir,"bmta",".geojson"))
conservancies <- st_read(paste0(dataDir,"conservancy",".geojson"))
control_one <- st_read(paste0(dataDir,"control1",".geojson"))
control_two <- st_read(paste0(dataDir,"control2",".geojson"))
gbr_non <- st_read(paste0(dataDir,"gbr_non",".geojson"))
parks <- st_read(paste0(dataDir,"parks",".geojson"))
reserves <- st_read(paste0(dataDir,"ecolres",".geojson"))
sfmas <- st_read(paste0(dataDir,"sfma",".geojson"))

#bind them all together and transform the crs
coast <- st_transform(dplyr::bind_rows(bmtas,conservancies,control_one,control_two,
                                       gbr_non,parks,reserves,sfmas),
                      3005)

```

Okay, so now we have a sf object that contains all of the coast. However, some of the geometries are a bit faulty, so we'll have to convert them. I will first split the data into (Multi-)polygons (the overwhelming part), and Geometry Collections (a lesser part), thereby eliminating points and linestrings. Then, I extract the polygon parts of the Geometry Collections, aggregate them into Multipolygons, and rejoin them with the other (Multi-)polygons.


```{r message=FALSE, warning=FALSE}
valid_part <- coast[st_is(coast,c("MULTIPOLYGON", "POLYGON")),]

geometry_collections <- coast[st_is(coast,"GEOMETRYCOLLECTION"),]

polygons_from_geometry_collections <- st_collection_extract(geometry_collections, "POLYGON")

unique_ids <- polygons_from_geometry_collections %>% 
  data.frame() %>% 
  select(-geometry) %>% 
  distinct()

new_multipolygons <- polygons_from_geometry_collections %>% 
  group_by(id) %>% 
  summarise() %>% 
  data.frame() %>% 
  left_join(., unique_ids, by='id')%>% 
  st_as_sf()

coast_final <- bind_rows(valid_part,new_multipolygons)
```

To illustrate the data, I will make two figures: One that displays the different land use zones, one that illustrates the data by showing relative forest cover at the beginning of the data (Figure 1 and 2 in the text respectively).

Land use zones:

```{r}
ggplot() +
  geom_sf(data = coast_final, aes(fill = Type), color = NA) +
  scale_fill_viridis_d(name = "Land Use Zones", option = "viridis", direction = 1,
                       labels = c("Biodiversity, Mining & Tourism Areas", "Conservancies", 
                                  "Other Coastal BC", "Ecological Reserve", 
                                  "Other Great Bear Rainforest", "Provincial Park",
                                  "Special Forest Management Area")) +
  labs(x = "Longitude", y= "Latitude")

#ggsave("C:/Users/Kamal/OneDrive/TSE/M2 EE/thesis/econ/graphics/land_use_zones.png", height = 8, width = 8)
```

Data illustration:

```{r}
coast_final %>% 
  mutate(area_ha = as.numeric(st_area(.)/10000)) %>% 
  mutate(weighted_forest_cover_2000 = as.numeric(cover2000*100/area_ha)) %>% 
  filter(weighted_forest_cover_2000 <= 100) %>% 
  ggplot() +
    geom_sf(aes(fill = weighted_forest_cover_2000)) + 
    scale_fill_viridis_c(name = "% Forest cover in 2000", option = "viridis", direction = -1) +
    labs(x = "Longitude", y = "Latitude")

#ggsave("C:/Users/Kamal/OneDrive/TSE/M2 EE/thesis/econ/graphics/forest_cover.png", height = 8, width = 8)
```

The following chunks exclude faulty cells (where there is more forest cover than area) and then forms an almost-ready data set.

```{r}
faulty_ids <- coast_final %>% 
  mutate(area_ha = as.numeric(st_area(.)/10000)) %>% 
  mutate(weighted_forest_cover_2010 = as.numeric(cover2010*100/area_ha)) %>% 
  filter(weighted_forest_cover_2010 > 100) %>% 
  pull(ID)

almost_ard <- coast_final %>% 
  dplyr::select(ID, Region, Zone, Type, geometry,starts_with("cover"),starts_with("loss")) %>% 
  pivot_longer(!c(ID, Region, Zone, Type, geometry), names_to = c("variable", "year"), 
               values_to = "value", names_pattern = "(\\w{4,})(\\d{4})") %>% 
  mutate(value = as.numeric(value), year = as.numeric(year)) %>% 
  # this makes sure that the cover in the previous year aligns with the loss in the current year
  mutate(year = case_when(variable == "cover" ~ (year + 1),
                          TRUE ~ as.numeric(year))) %>%
  pivot_wider(names_from = "variable") %>% 
  arrange(ID,year) %>%
  relocate(year, cover, loss, Type, .after = ID) %>% 
  rename(id = ID, prev_cover = cover, type = Type, region = Region, zone = Zone) %>% 
  mutate(gbr = ifelse(type == "Control",0,1),.before = type) %>% 
  filter(!id %in% faulty_ids) 
```

The next code chunk adds cell-year specific winter temperatures. It imports the webscraped data sets and extracts the lowest monthly minimum temperature (Â°C) for December and January for each year:


```{r message=FALSE, warning=FALSE}
temp <- read_csv("C:/Users/Kamal/OneDrive/TSE/M2 EE/thesis/econ/data/climate/en_climate_summaries_BC_12-2000.csv") %>% 
  select(Long,Lat,Clim_ID,Tn) %>% 
  mutate(year = 2000) %>% 
  mutate(month = 12)

for(i in 2001:2021){
  
  for(j in c("01","12")){
    
    import <- read_csv(paste0("C:/Users/Kamal/OneDrive/TSE/M2 EE/thesis/econ/data/climate/en_climate_summaries_BC_",
                              j,"-",i,".csv")) %>% 
      select(Long,Lat,Clim_ID,Tn) %>% 
      mutate(year = i) %>% 
      mutate(month = j)
    
    temp <- rbind(temp,import)
    
  }
}

unique_clim_ids <- temp %>% 
  select(Clim_ID, Long, Lat) %>% 
  distinct() %>% 
  summarize(longitude = mean(Long), latitude = mean(Lat), .by = Clim_ID)

mintemp <- temp %>% 
  left_join(unique_clim_ids, by = "Clim_ID") %>% 
  select(-Long,-Lat) %>% 
  rename(station = Clim_ID, mintemp = Tn) %>% 
  mutate(year = case_when(month == "12" ~ year + 1, TRUE ~ year)) %>% 
  summarize(mintemp = mean(mintemp, na.rm = T),
            .by = c(station,year,longitude,latitude)) %>% 
  pivot_wider(names_from = year, values_from = mintemp, names_prefix = "mintemp_") %>% 
  filter_at(vars(starts_with("mintemp_")), any_vars(! is.na(.))) %>% 
  st_as_sf(.,coords = c("longitude","latitude"))
```

This code chunk finds the three nearest neighbors (three nearest weather stations) for each polygon, and averages over those.

```{r message=FALSE, warning=FALSE}
library(nngeo)

main_wide <- almost_ard %>% 
  rename(prevcover = prev_cover) %>% 
  data.frame() %>% 
  pivot_wider(names_from = year, values_from = c(prevcover,loss), names_sep = "_") %>% 
  st_as_sf()

mintemp <- st_set_crs(mintemp,4269)
mintemp <- st_transform(mintemp, st_crs(main_wide))


neighbors <- st_nn(main_wide,mintemp,k=3)

append <- data.frame(matrix(ncol = 21))
colnames(append) <- mintemp %>% data.frame() %>% select(2:22) %>% colnames()

for(i in 1:length(neighbors)){
  
  append <- rbind(append, 
                  data.frame(mintemp[neighbors[[i]],2:22]) %>% select(-geometry) %>% colMeans(na.rm = T))
}


append <- append[2:3973,]

append %>% filter_at(vars(starts_with("mintemp_")), any_vars(! is.na(.)))
  

```

This was successful: no cell has no weather data at all.

```{r message=FALSE, warning=FALSE}
ard <- append %>%  
  cbind(main_wide,.) %>% 
  data.frame() %>% 
  pivot_longer(cols = starts_with(c("prevcover","loss","mintemp")), names_to = c("variable", "year"),
               values_to = "value", names_sep = "_") %>% 
  pivot_wider(names_from = "variable", values_from = "value") %>% 
  filter(year != 2022) %>% 
  relocate(year,prevcover,loss,mintemp,.before = gbr) %>% 
  st_as_sf()

```

This data set is now ready to be processed.  Here is the export:

```{r}
#st_write(ard,"C:/Users/Kamal/OneDrive/TSE/M2 EE/thesis/econ/data/ard_luz_clim_full.shp")
```
